{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Notebook Fun for Lessons 1/2\n",
    "\n",
    "Note: the session notebooks are not reviewed and only serve for the purpose of in-class illustrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Checking a few things about cross-entropy\n",
    "\n",
    "Imagine a probability distribution $p$ cross 5 outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3  0.2  0.1  0.25 0.15]\n"
     ]
    }
   ],
   "source": [
    "p = np.array([0.3,0.2,0.1,0.25,0.15])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.228212945841001\n"
     ]
    }
   ],
   "source": [
    "S = -(0.3 * np.log2(0.3) + 0.2 * np.log2(0.2) + 0.1 * np.log2(0.1) + 0.25 * np.log2(0.25) + 0.15 * np.log2(0.15))\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit, but tedious and silly. Better way? Use vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.228212945841001\n"
     ]
    }
   ],
   "source": [
    "S = - np.dot(p, np.log2(p))\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result. Good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine a 2nd distribution q which is close to $p$ but differs a bit. \n",
    "Construct it via a small $\\delta$ vector which has a mean of zero, and whose scale is a small number $\\epsilon$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:  [0.29531147 0.20553361 0.09956972 0.24076992 0.15881527]\n"
     ]
    }
   ],
   "source": [
    "def create_q(p0, epsilon=0.03):\n",
    "\n",
    "    delta = epsilon * np.random.random(5)\n",
    "    delta = delta - np.mean(delta)\n",
    "    \n",
    "    return (p0 + delta)\n",
    "\n",
    "q = create_q(p)\n",
    "print('q: ', q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the cross entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2289878623320147\n"
     ]
    }
   ],
   "source": [
    "ce = -np.dot(p, np.log2(q))\n",
    "print(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close! Smaller or bigger than entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print((ce - S) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that for many q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce > S?  True\n",
      "ce > S?  True\n",
      "ce > S?  True\n",
      "ce > S?  True\n",
      "ce > S?  True\n"
     ]
    }
   ],
   "source": [
    "for attempt in range(5):\n",
    "    q = create_q(p)\n",
    "    ce = -np.dot(p, np.log2(q))\n",
    "    #print(q)\n",
    "    print('ce > S? ', (ce-S)>0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed... $ce$ is always larger than $S$ ('experimentally verified', but should be proven). $ce = S$ when $q=p$, therefor minimizing $ce$ drives $q \\rightarrow p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A few basic words on vector and matrix calculations \n",
    "\n",
    "Imagine a neural net where the dimension  of the incoming layer is $4$, and the next layer has dimension $3$. Let $x$ be the input. What are the dimensions of the weight matrix $W$ and bias vector $b$ in $$z = f(x W + b)?$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ needs to 'translate' a 4-d vector into a 3-d vector. Hence dim$(W) = 4 \\times 3$.\n",
    "\n",
    "Each output neuron has its own bias value, dim$(b) = 3$.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,1,2,2])\n",
    "\n",
    "W = np.array([[-1,-3,-2],[-2,3,-6],[4,-2,3], [-1, 5, 1]]) \n",
    "\n",
    "\n",
    "b = np.array([-1, -2, -3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (4,)\n",
      "W shape:  (4, 3)\n",
      "b shape:  (3,)\n"
     ]
    }
   ],
   "source": [
    "print('x shape: ', x.shape)\n",
    "print('W shape: ', W.shape)\n",
    "print('b shape: ', b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  4, -3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.dot(W) + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the non-linearity, sigmoid to be concrete. The non-linearity is an element-wise operation. (The last output layer at the end however is different.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    return 1/(1 + np.exp(-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88079708, 0.98201379, 0.04742587])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = sigmoid(z)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Familiarization with the Output Layer and Softmax operation\n",
    "\n",
    "Imagine your output layer (which does not have a non-linearity) returns these 5 numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o:  [ 4.23717353 -0.1165268  -4.15928131 -4.72762245 -0.39322557]\n"
     ]
    }
   ],
   "source": [
    "o = 10 * np.random.random(5) - 5\n",
    "print('o: ', o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification problem, these 5 numbers would be the output for each class. They can be large, small, positive, negative, etc. \n",
    "\n",
    "We want to model probabilities though. Hence, we need to convert the output to 5 values that are **1) positive** and **2) sum to 1**.\n",
    "\n",
    "Let's first exponentiate all values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  [6.92119495e+01 8.90006248e-01 1.56187790e-02 8.84748135e-03\n",
      " 6.74876498e-01]\n"
     ]
    }
   ],
   "source": [
    "exp_o = np.exp(o)\n",
    "print('outputs: ', exp_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. All numbers are positive. But they don't sum to $1$. Simple solution: divide each value by the sum of all values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of all output values:  70.80129855521079\n"
     ]
    }
   ],
   "source": [
    "sum_exp_o = np.sum(exp_o)\n",
    "print('sum of all output values: ', sum_exp_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model probabilities p:  [9.77551979e-01 1.25704792e-02 2.20600177e-04 1.24962134e-04\n",
      " 9.53197910e-03]\n"
     ]
    }
   ],
   "source": [
    "p = exp_o/sum_exp_o\n",
    "print('model probabilities p: ', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model probabilities q:  [9.77551979e-01 1.25704792e-02 2.20600177e-04 1.24962134e-04\n",
      " 9.53197910e-03]\n"
     ]
    }
   ],
   "source": [
    "p = exp_o/sum_exp_o\n",
    "print('model probabilities q: ', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Sums to 1 as desired. (Not a surprise.)\n",
    "\n",
    "What happens if I add a constant to each value in $o$? How is softmax affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_2 = o + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  [1.88137585e+02 2.41928781e+00 4.24562432e-02 2.40499478e-02\n",
      " 1.83450452e+00]\n"
     ]
    }
   ],
   "source": [
    "exp_o_2 = np.exp(o_2)\n",
    "print('outputs: ', exp_o_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_exp_o_2 = np.sum(exp_o_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new probabilities q_2:  [9.77551979e-01 1.25704792e-02 2.20600177e-04 1.24962134e-04\n",
      " 9.53197910e-03]\n"
     ]
    }
   ],
   "source": [
    "q = exp_o_2/sum_exp_o_2\n",
    "print('new probabilities q_2: ', q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Most Basic Keras Intro\n",
    "\n",
    "Simple 3-class classification model. This is only intended to illustrate the TensorFlow/Keras concepts. The data is randomly generated. See: https://www.tensorflow.org/guide/keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of formalisms: 1) **Sequential** models, and 2) **Functional API**. The former is a little simpler, but it only supports (as the name says) sequential architectures. The Functional API approach is extremely flexible and will be introduced later.   \n",
    "\n",
    "\n",
    "Setting up a Sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'sequential' model (vs. 'functional'... we'll discuss later.)\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "# Adds a densely-connected layer with 8 units to the model:\n",
    "layers.Dense(8, activation='relu',\n",
    "             kernel_initializer=tf.keras.initializers.glorot_normal ,\n",
    "             input_shape=(4,)),         # '4' is the number of features of the input\n",
    "    \n",
    "# Add another, now with 15 neurons:\n",
    "layers.Dense(15, activation='relu'),\n",
    "    \n",
    "# Add another, with 5 neurons:\n",
    "layers.Dense(5, activation='relu'),\n",
    "    \n",
    "# Create a softmax layer with 3 output units - as required by the labels being 3 dimensional representinbg 3 classes:\n",
    "layers.Dense(3, activation='softmax')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model... i.e., adding losses and metrics etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a model for categorical classification.\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.categorical_accuracy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some fake input data ():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_one_hot_labels(shape):\n",
    "    n, n_class = shape\n",
    "    classes = np.random.randint(0, n_class, n)\n",
    "    labels = np.zeros((n, n_class))\n",
    "    labels[np.arange(n), classes] = 1\n",
    "    return labels\n",
    "\n",
    "data = np.random.random((1000, 4))\n",
    "labels = random_one_hot_labels((1000, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15491145, 0.1624717 , 0.77169716, 0.86330761],\n",
       "       [0.32929663, 0.90288679, 0.73958897, 0.45347842],\n",
       "       [0.99768954, 0.32197616, 0.38582041, 0.67262834]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then train your model. (well... nothing to be trained here as there are by construction no patterns.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "1000/1000 - 0s - loss: 1.0993 - categorical_accuracy: 0.3450\n",
      "Train on 1000 samples\n",
      "1000/1000 - 0s - loss: 0.9331 - categorical_accuracy: 0.5350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13aec7150>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, labels,   epochs=1, batch_size=1000, verbose=2)  # verbose=2: only show last step \n",
    "model.fit(data, labels,   epochs=1000, batch_size=50, verbose=0) # verbose=0: silent execution\n",
    "model.fit(data, labels,   epochs=1, batch_size=1000, verbose=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the value of the initial cost function surprising? Shouldn't..:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1086626245216111"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we don't expect much training for random data. Any training here is over-training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_1",
   "language": "python",
   "name": "tf2_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
